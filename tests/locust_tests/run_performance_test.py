#!/usr/bin/env python3
"""
ORM ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ê° ORMë³„ë¡œ ë™ì¼í•œ ì¡°ê±´ì—ì„œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
"""

import subprocess
import time
import json
import os
from datetime import datetime
from pathlib import Path

class PerformanceTestRunner:
    def __init__(self):
        self.test_config = {
            "users": 20,  # RPS 20ì— ë§ì¶¤
            "spawn_rate": 5,  # ì´ˆë‹¹ 5ëª…ì”© ìƒì„±
            "run_time": "2m",  # 2ë¶„ê°„ í…ŒìŠ¤íŠ¸
            "processes": 4  # ì›Œì»¤ 4ê°œ
        }
        
        self.orm_configs = {
            "sqlalchemy": {
                "port": 8001,
                "name": "SQLAlchemy v2",
                "description": "greenletì„ ì‚¬ìš©í•œ ë™ê¸°â†’ë¹„ë™ê¸° ë³€í™˜"
            },
            "tortoise": {
                "port": 8002,
                "name": "Tortoise ORM",
                "description": "ë„¤ì´í‹°ë¸Œ ë¹„ë™ê¸° ORM"
            },
            "edgedb": {
                "port": 8003,
                "name": "EdgeDB",
                "description": "ì°¨ì„¸ëŒ€ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤"
            }
        }
        
        self.results = {}

    def check_server_health(self, port):
        """ì„œë²„ ìƒíƒœ í™•ì¸"""
        import requests
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=5)
            return response.status_code == 200
        except:
            return False

    def run_locust_test(self, orm_name, port):
        """íŠ¹ì • ORMì— ëŒ€í•´ Locust í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"\n{'='*50}")
        print(f"Testing {self.orm_configs[orm_name]['name']}")
        print(f"Description: {self.orm_configs[orm_name]['description']}")
        print(f"Port: {port}")
        print(f"{'='*50}")
        
        # ì„œë²„ ìƒíƒœ í™•ì¸
        if not self.check_server_health(port):
            print(f"âŒ Server at port {port} is not responding. Skipping {orm_name}.")
            return None
        
        # Locust ëª…ë ¹ì–´ êµ¬ì„±
        cmd = [
            "locust",
            "-f", "tests/locust_tests/locust_test.py",
            f"--host=http://localhost:{port}",
            f"--users={self.test_config['users']}",
            f"--spawn-rate={self.test_config['spawn_rate']}",
            f"--run-time={self.test_config['run_time']}",
            f"--processes={self.test_config['processes']}",
            "--headless",
            "--print-stats",
            "--only-summary",
            f"--html=results/{orm_name}_report.html",
            f"--csv=results/{orm_name}"
        ]
        
        print(f"Running command: {' '.join(cmd)}")
        
        try:
            # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs("results", exist_ok=True)
            
            # Locust ì‹¤í–‰
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                print(f"âœ… {orm_name} test completed successfully")
                
                # ê²°ê³¼ íŒŒì‹±
                output_lines = result.stdout.split('\n')
                stats = self.parse_locust_output(output_lines)
                stats['orm'] = orm_name
                stats['description'] = self.orm_configs[orm_name]['description']
                
                return stats
            else:
                print(f"âŒ {orm_name} test failed")
                print(f"Error: {result.stderr}")
                return None
                
        except subprocess.TimeoutExpired:
            print(f"âŒ {orm_name} test timed out")
            return None
        except Exception as e:
            print(f"âŒ {orm_name} test error: {e}")
            return None

    def parse_locust_output(self, output_lines):
        """Locust ì¶œë ¥ì—ì„œ í†µê³„ íŒŒì‹±"""
        stats = {
            "total_requests": 0,
            "failures": 0,
            "avg_response_time": 0,
            "p95_response_time": 0,
            "p99_response_time": 0,
            "rps": 0,
            "failure_rate": 0
        }
        
        # ì¶œë ¥ì—ì„œ í†µê³„ ì¶”ì¶œ (ì‹¤ì œ Locust ì¶œë ¥ í˜•ì‹ì— ë§ê²Œ ì¡°ì • í•„ìš”)
        for line in output_lines:
            if "Total requests" in line or "Aggregated" in line:
                # ìš”ì²­ ìˆ˜, ì‹¤íŒ¨ìœ¨, ì‘ë‹µì‹œê°„ ë“± íŒŒì‹±
                # ì‹¤ì œ êµ¬í˜„ì‹œ ì •í™•í•œ íŒŒì‹± ë¡œì§ í•„ìš”
                pass
        
        return stats

    def run_all_tests(self):
        """ëª¨ë“  ORMì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ Starting ORM Performance Tests")
        print(f"Configuration: {self.test_config}")
        print(f"Timestamp: {datetime.now().isoformat()}")
        
        for orm_name, config in self.orm_configs.items():
            result = self.run_locust_test(orm_name, config["port"])
            if result:
                self.results[orm_name] = result
            
            # í…ŒìŠ¤íŠ¸ ê°„ ëŒ€ê¸°
            time.sleep(10)
        
        # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
        self.save_results()
        self.print_comparison()

    def save_results(self):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"results/performance_comparison_{timestamp}.json"
        
        os.makedirs("results", exist_ok=True)
        
        with open(filename, 'w') as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "test_config": self.test_config,
                "results": self.results
            }, f, indent=2)
        
        print(f"\nğŸ“Š Results saved to: {filename}")

    def print_comparison(self):
        """ê²°ê³¼ ë¹„êµ ì¶œë ¥"""
        if not self.results:
            print("âŒ No results to compare")
            return
        
        print("\n" + "="*80)
        print("ğŸ† PERFORMANCE COMPARISON RESULTS")
        print("="*80)
        
        # í—¤ë”
        print(f"{'ORM':<15} {'P95 (ms)':<10} {'RPS':<8} {'Failure Rate':<12} {'Description'}")
        print("-" * 80)
        
        # ê° ORM ê²°ê³¼ ì¶œë ¥
        for orm_name, stats in self.results.items():
            print(f"{stats['orm'].upper():<15} "
                  f"{stats.get('p95_response_time', 'N/A'):<10} "
                  f"{stats.get('rps', 'N/A'):<8} "
                  f"{stats.get('failure_rate', 'N/A'):<12} "
                  f"{stats['description']}")
        
        print("="*80)
        
        # ìŠ¹ì ê²°ì • (P95 ê¸°ì¤€)
        if len(self.results) > 1:
            p95_results = {orm: stats.get('p95_response_time', float('inf')) 
                          for orm, stats in self.results.items() 
                          if isinstance(stats.get('p95_response_time'), (int, float))}
            
            if p95_results:
                winner = min(p95_results.keys(), key=lambda x: p95_results[x])
                print(f"ğŸ† P95 ì‘ë‹µì‹œê°„ ê¸°ì¤€ ìš°ìŠ¹ì: {winner.upper()}")
                print(f"   ì‘ë‹µì‹œê°„: {p95_results[winner]:.2f}ms")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ ORM Performance Test Runner")
    print("Make sure all servers are running before starting tests:")
    print("  - SQLAlchemy: http://localhost:8001")
    print("  - Tortoise:   http://localhost:8002") 
    print("  - EdgeDB:     http://localhost:8003")
    print()
    
    input("Press Enter to start tests...")
    
    runner = PerformanceTestRunner()
    runner.run_all_tests()

if __name__ == "__main__":
    main() 